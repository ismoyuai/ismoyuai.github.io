<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Ollama调用本地模型</title>
      <link href="/ollama-diao-yong-ben-di-mo-xing/"/>
      <url>/ollama-diao-yong-ben-di-mo-xing/</url>
      
        <content type="html"><![CDATA[<h1 id="Linux安装Ollama"><a href="#Linux安装Ollama" class="headerlink" title="Linux安装Ollama"></a>Linux安装Ollama</h1><h2 id="安装命令："><a href="#安装命令：" class="headerlink" title="安装命令："></a>安装命令：</h2><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl -fsSL https://ollama.com/install.sh | sh</span><br></pre></td></tr></tbody></table></figure><p>如果下载速度很慢，可以开启学术加速</p><h3 id="开启学术加速"><a href="#开启学术加速" class="headerlink" title="开启学术加速:"></a>开启学术加速:</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> /etc/network_turbo</span><br></pre></td></tr></tbody></table></figure><h3 id="取消学术加速"><a href="#取消学术加速" class="headerlink" title="取消学术加速:"></a>取消学术加速:</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">unset</span> http_proxy &amp;&amp; <span class="built_in">unset</span> https_proxy</span><br></pre></td></tr></tbody></table></figure><h2 id="Ollama安装模型："><a href="#Ollama安装模型：" class="headerlink" title="Ollama安装模型："></a>Ollama安装模型：</h2><h3 id="下载模型"><a href="#下载模型" class="headerlink" title="下载模型"></a>下载模型</h3><p>以 llama3.2:1b 模型为例，运行下面命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run llama3.2:1b</span><br></pre></td></tr></tbody></table></figure><p>等待命令结束。</p><h3 id="使用模型"><a href="#使用模型" class="headerlink" title="使用模型"></a>使用模型</h3><p>ollama使用模型有两种方式，一种通过终端命令使用，一种通过python代码使用，两种方式都需要首先开启ollama服务</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama serve</span><br></pre></td></tr></tbody></table></figure><h4 id="1-通过终端使用"><a href="#1-通过终端使用" class="headerlink" title="1.通过终端使用"></a>1.通过终端使用</h4><p>终端运行命令：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ollama run llama3.2:1b</span><br></pre></td></tr></tbody></table></figure><p>终端会进入对话模式，这时输入你要问的问题即可使用模型。</p><h4 id="2-通过python代码使用模型"><a href="#2-通过python代码使用模型" class="headerlink" title="2.通过python代码使用模型"></a>2.通过python代码使用模型</h4><p>安装 openai 包</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install opeani</span><br></pre></td></tr></tbody></table></figure><p>python代码简单调用</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过openai接口调用模型，导入模型地址，和api_key</span></span><br><span class="line">client = OpenAI(base_url=<span class="string">"http://localhost:11434/v1/"</span>,api_key=<span class="string">"ollama"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 提示词模板，并输入用户问题，指定使用模型名字</span></span><br><span class="line">responce=client.chat.completions.create(</span><br><span class="line">    messages= [{<span class="string">"role"</span>:<span class="string">"user"</span>,<span class="string">"content"</span>:<span class="string">"你好!你是谁？你是由谁创造的？"</span>}],model=<span class="string">"llama3.2:1B"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(responce.choices[<span class="number">0</span>])</span><br></pre></td></tr></tbody></table></figure><p>base_url：启动<code>ollama serve</code>后会给出地址和端口，一般为<a href="http://localhost:11434/">http://localhost:11434</a></p><p>api_key：为ollama，因为使用的ollama框架</p><p>简单多轮对话框架调用：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#多轮对话</span></span><br><span class="line"><span class="keyword">from</span> openai <span class="keyword">import</span> OpenAI</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义多轮对话方法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">run_chat_session</span>():</span><br><span class="line">    <span class="comment">#初始化客户端</span></span><br><span class="line">    client = OpenAI(base_url=<span class="string">"http://localhost:11434/v1/"</span>,api_key=<span class="string">"ollama"</span>)</span><br><span class="line">    <span class="comment">#初始化对话历史</span></span><br><span class="line">    chat_history = []</span><br><span class="line">    <span class="comment">#启动多轮对话</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment">#获取用户输入</span></span><br><span class="line">        user_input = <span class="built_in">input</span>(<span class="string">"用户："</span>)</span><br><span class="line">        <span class="keyword">if</span> user_input.lower()==<span class="string">"exit"</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"退出对话"</span>)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment">#更新对话历史（添加用户输入）</span></span><br><span class="line">        chat_history.append({<span class="string">"role"</span>:<span class="string">"user"</span>,<span class="string">"content"</span>:user_input})</span><br><span class="line">        <span class="comment">#调用模型回答</span></span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            chat_complition = client.chat.completions.create(messages=chat_history,model=<span class="string">"llama3.2:1b"</span>)</span><br><span class="line">            <span class="comment">#获取最新回答</span></span><br><span class="line">            moedl_responce = chat_complition.choices[<span class="number">0</span>]</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"AI："</span>,moedl_responce.message.content)</span><br><span class="line">            <span class="comment">#更新对话历史（添加AI模型的回复）</span></span><br><span class="line">            chat_history.append({<span class="string">"role"</span>:<span class="string">"assistant"</span>,<span class="string">"content"</span>:moedl_responce.message.content})</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"发生错误："</span>,e)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    run_chat_session()</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> AI大模型应用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ollama </tag>
            
            <tag> python </tag>
            
            <tag> linux </tag>
            
            <tag> LLaMa </tag>
            
            <tag> LoRA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LLaMa3微调</title>
      <link href="/llama3-wei-diao/"/>
      <url>/llama3-wei-diao/</url>
      
        <content type="html"><![CDATA[<h1 id="LLaMa3微调"><a href="#LLaMa3微调" class="headerlink" title="LLaMa3微调"></a>LLaMa3微调</h1><h2 id="LoRA"><a href="#LoRA" class="headerlink" title="LoRA"></a>LoRA</h2><h3 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h3><p>LoRA（Low-Rank Adaptation）是一种用于大模型微调的技术，通过引入低秩矩阵来减少微调时的参数量。在预训练的模型中，LoRA通过添加两个小矩阵B和A来近似原始的大矩阵ΔW，从而减少需要更新的参数数量。具体来说，LoRA通过将全参微调的增量 参数矩阵ΔW表示为两个参数量更小的矩阵B和A的低秩近似来实现：</p><p>[ W_0 + \Delta W = W_0 + BA ] </p><p>其中，B和A的秩远小于原始矩阵的秩，从而大大减少了需要更新的参数数量。</p><h3 id="LoRA思想"><a href="#LoRA思想" class="headerlink" title="LoRA思想"></a>LoRA思想</h3><p>预训练模型中存在一个极小的内在维度，这个内在维度是发挥核心作用的地方。在继续训练的过程中，权重的更新依然也有如此特点，即也存在一个内在维度(内在秩)</p><p>权重更新：W=W+^W</p><p>因此，可以通过矩阵分解的方式，将原本要更新的大的矩阵变为两个小的矩阵</p><p>权重更新：W=W+^W=W+BA</p><p>具体做法，即在矩阵计算中增加一个旁系分支，旁系分支由两个 低秩矩阵A和B组成</p><h3 id="LoRA原理"><a href="#LoRA原理" class="headerlink" title="LoRA原理"></a>LoRA原理</h3><p>训练时，输入分别与原始权重和两个低秩矩阵进行计算，共同得 到最终结果，优化则仅优化A和B。</p><p>训练完成后，可以将两个低秩矩阵与原始模型中的权重进行合并， 合并后的模型与原始模型无异</p><h2 id="LLaMA-Factory"><a href="#LLaMA-Factory" class="headerlink" title="LLaMA-Factory"></a>LLaMA-Factory</h2><p>1.安装</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/hiyouga/LLaMA-Factory.git</span><br><span class="line"><span class="built_in">cd</span> LLaMA-Factory</span><br><span class="line">pip install -e .</span><br></pre></td></tr></tbody></table></figure><p>2.准备训练数据</p><p>例：</p><p>训练数据：</p><ul><li>fintech.json</li><li>identity.json</li></ul><p>将训练数据放在目录 LLaMA-Factory/data/fintech.json </p><p>并且修改数据注册文件：LLaMA-Factory/data/dataset_info.json</p><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">"fintech"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line"><span class="attr">"file_name"</span><span class="punctuation">:</span> <span class="string">"fintech.json"</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">"columns"</span><span class="punctuation">:</span> <span class="punctuation">{</span></span><br><span class="line"><span class="attr">"prompt"</span><span class="punctuation">:</span> <span class="string">"instruction"</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">"query"</span><span class="punctuation">:</span> <span class="string">"input"</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">"response"</span><span class="punctuation">:</span> <span class="string">"output"</span><span class="punctuation">,</span></span><br><span class="line"><span class="attr">"history"</span><span class="punctuation">:</span> <span class="string">"history"</span></span><br><span class="line"><span class="punctuation">}</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></tbody></table></figure><h3 id="通过界面进行训练"><a href="#通过界面进行训练" class="headerlink" title="通过界面进行训练"></a>通过界面进行训练</h3><p>启动Web UI</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd LLaMA-Factory</span><br><span class="line">llamafactory-cli webui</span><br></pre></td></tr></tbody></table></figure><p>模型微调</p><ul><li>使用 Web UI 训练</li><li>使用命令行执行</li></ul><h4 id="问题记录："><a href="#问题记录：" class="headerlink" title="问题记录："></a>问题记录：</h4><p>我在实际安装过程中由于懒得租服务器和安装Linux双系统，所以就直接使用Windows11系统来安装配置LLaMA-Factory，按照上面步骤配置好后，成功启动了webui界面，但是在配置好训练参数后，点击开始训练，结果报错：未找到cuda环境，随后程序报错中断服务了</p><p><img src="C:\Users\25423\Desktop\QQ截图20250423165837.png" alt="QQ截图20250423165837"></p><p>原因推测是没有在使用的python环境中安装 CUDA + pytorch 环境导致的，或者安装的环境版本不对，安装 CUDA + pytorch 包</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conda install pytorch==2.1.0 torchvision==0.16.0 torchaudio==2.1.0 pytorch-cuda=12.1 -c pytorch -c nvidia</span><br></pre></td></tr></tbody></table></figure><p>后续没有解决，准备还是在服务器上进行测试。</p><h3 id="通过配置文件开始训练"><a href="#通过配置文件开始训练" class="headerlink" title="通过配置文件开始训练"></a>通过配置文件开始训练</h3><p>配置文件位于：[cust/train_llama3_lora_sft.yaml] </p><p>构建 cust/train_llama3_lora_sft.yaml</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">cutoff_len: 1024</span><br><span class="line"> dataset: fintech,identity</span><br><span class="line"> dataset_dir: data</span><br><span class="line"> do_train: true</span><br><span class="line"> finetuning_type: lora</span><br><span class="line"> flash_attn: auto</span><br><span class="line"> fp16: true</span><br><span class="line"> gradient_accumulation_steps: 8</span><br><span class="line"> learning_rate: 0.0002</span><br><span class="line"> logging_steps: 5</span><br><span class="line"> lora_alpha: 16</span><br><span class="line"> lora_dropout: 0</span><br><span class="line"> lora_rank: 8</span><br><span class="line"> lora_target: q_proj,v_proj</span><br><span class="line"> lr_scheduler_type: cosine</span><br><span class="line"> max_grad_norm: 1.0</span><br><span class="line"> max_samples: 1000</span><br><span class="line"> model_name_or_path: /root/autodl-tmp/models/Llama3-8B-Chinese-Chat</span><br><span class="line"> num_train_epochs: 10.0</span><br><span class="line"> optim: adamw_torch</span><br><span class="line"> output_dir: saves/LLaMA3-8B-Chinese-Chat/lora/train_2024-05-25-20-27-47</span><br><span class="line"> packing: false</span><br><span class="line"> per_device_train_batch_size: 2</span><br><span class="line"> plot_loss: true</span><br><span class="line"> preprocessing_num_workers: 16</span><br><span class="line"> report_to: none</span><br><span class="line"> save_steps: 100</span><br><span class="line"> stage: sft</span><br><span class="line"> template: llama3</span><br><span class="line"> use_unsloth: true</span><br><span class="line"> warmup_steps: 0</span><br></pre></td></tr></tbody></table></figure><p>命令行执行：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">llamafactory-cli train cust/train_llama3_lora_sft.yaml</span><br></pre></td></tr></tbody></table></figure><p>案例:自定义数据集完成LLaMA3微调训练</p>]]></content>
      
      
      <categories>
          
          <category> AI大模型应用 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ollama </tag>
            
            <tag> python </tag>
            
            <tag> linux </tag>
            
            <tag> LLaMa </tag>
            
            <tag> LoRA </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
